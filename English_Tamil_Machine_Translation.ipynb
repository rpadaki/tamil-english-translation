{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "English-Tamil Machine Translation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rpadaki/tamil-english-translation/blob/master/English_Tamil_Machine_Translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SHnUvELyybK",
        "colab_type": "code",
        "outputId": "6d57e5b2-d19d-42d0-d210-58def25849b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pip install -q torch torchtext opt_einsum\n",
        "!pip install -qU git+https://github.com/harvardnlp/namedtensor"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Building wheel for namedtensor (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wP6jAob6oWI",
        "colab_type": "code",
        "outputId": "210b652e-0ca2-44d5-f394-b7a504081296",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "!curl http://ufal.mff.cuni.cz/~ramasamy/parallel/data/v2/en-ta-parallel-v2.tar.gz | tar xvz"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0en-ta-parallel-v2/\n",
            "en-ta-parallel-v2/corpus.bcn.dev.en\n",
            "en-ta-parallel-v2/corpus.bcn.dev.ta\n",
            "  0 23.7M    0  126k    0     0   183k      0  0:02:12 --:--:--  0:02:12  183ken-ta-parallel-v2/corpus.bcn.test.en\n",
            "en-ta-parallel-v2/corpus.bcn.test.ta\n",
            "en-ta-parallel-v2/corpus.bcn.train.en\n",
            "en-ta-parallel-v2/corpus.bcn.train.ta\n",
            "100 23.7M  100 23.7M    0     0  10.4M      0  0:00:02  0:00:02 --:--:-- 10.4M\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4y_V4QsOy06K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchtext\n",
        "from torchtext.vocab import Vectors\n",
        "from torchtext import data, datasets\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from namedtensor import ntorch\n",
        "from namedtensor.text import NamedField\n",
        "\n",
        "import numpy as np\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anaQd-ijzdEN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize_ta(text):\n",
        "    sent = text.split()\n",
        "    sent.reverse()\n",
        "    return sent\n",
        "\n",
        "def tokenize_en(text):\n",
        "    return text.split()\n",
        "\n",
        "BOS_WORD = \"<s>\"\n",
        "EOS_WORD = \"</s>\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKYXv1_Uz4Pd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TA = NamedField(names=(\"seqlen\",), tokenize=tokenize_ta)\n",
        "EN = NamedField(names=(\"seqlen\",), tokenize=tokenize_en,\n",
        "                init_token=BOS_WORD, eos_token=EOS_WORD)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyhIkxr8BrHz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LEN = 20\n",
        "MIN_FREQ = 7"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nETU6VHo21dd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train, val, test = datasets.TranslationDataset.splits(\n",
        "     exts=('.ta', '.en'),\n",
        "     fields=(TA, EN),\n",
        "     path=\"./en-ta-parallel-v2\",\n",
        "     train=\"corpus.bcn.train\",\n",
        "     test=\"corpus.bcn.test\",\n",
        "     validation=\"corpus.bcn.dev\",\n",
        "     filter_pred = lambda x: len(vars(x)['src']) <= MAX_LEN and len(vars(x)['trg']) <= MAX_LEN\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "af85p-ai6fcC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TA.build_vocab(train.src, min_freq=MIN_FREQ)\n",
        "EN.build_vocab(train.trg, min_freq=MIN_FREQ)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PON_CxvLmqYC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BOS_ID = EN.vocab.stoi[BOS_WORD]\n",
        "EOS_ID = EN.vocab.stoi[EOS_WORD]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34AB2EJ5Ci2H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda')\n",
        "train_iter, val_iter, test_iter = data.BucketIterator.splits(\n",
        "    (train, val, test),\n",
        "    batch_size=32, \n",
        "    device=device,\n",
        "    repeat=False\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QKxbEdEEHno",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def inspect(b, s):\n",
        "    batch = [i for i in iter(test_iter)][b]\n",
        "    return \" \".join([TA.vocab.itos[i] for i in batch.src[{\"batch\": s}].values.data]), \" \".join([EN.vocab.itos[i] for i in batch.trg[{\"batch\": s}].values.data])\n",
        "\n",
        "def get_tamil_tensor(b, s):\n",
        "    batch = [i for i in iter(test_iter)][b]\n",
        "    return batch.src[{\"batch\": s}]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iK4IFWo9i-h3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EmbeddingLM(ntorch.nn.Module):\n",
        "    def __init__(self, TEXT, dropout=0.0, max_embedding_norm=None,\n",
        "                 embedding_size=1000):\n",
        "        super(EmbeddingLM, self).__init__()\n",
        "        self.dropout_prob = dropout\n",
        "        self.dropout = ntorch.nn.Dropout(self.dropout_prob)\n",
        "        \n",
        "        self.vocab_size = len(TEXT.vocab)\n",
        "        self.embedding_dim = embedding_size\n",
        "        self.embeddings = ntorch.nn.Embedding(self.vocab_size,\n",
        "                                              self.embedding_dim,\n",
        "                                              max_norm=max_embedding_norm)\n",
        "        self.embeddings.spec(\"seqlen\", \"embedding\")\n",
        "#         self.embeddings._output_augment = \"embedding\"\n",
        "#         self.embeddings._input_order = ()\n",
        "#         self.embeddings._spec = True\n",
        "     "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7omhvAlWRklA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderLSTM(EmbeddingLM):\n",
        "    def __init__(self, TEXT, hidden_size=500, num_layers=2,\n",
        "                 **kwargs):\n",
        "        super(EncoderLSTM, self).__init__(TEXT, **kwargs)\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = ntorch.nn.LSTM(input_size=self.embedding_dim,\n",
        "                                   hidden_size=self.hidden_size,\n",
        "                                   num_layers=self.num_layers,\n",
        "                                   dropout=self.dropout_prob,\n",
        "                                   batch_first=True)\n",
        "        self.lstm.spec(\"embedding\", \"seqlen\", \"encoding\")\n",
        "    \n",
        "    def forward(self, input):\n",
        "        x = self.embeddings(input)\n",
        "        x = self.dropout(x)\n",
        "        output, (hidden, context) = self.lstm(x)\n",
        "        return hidden, context"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNYRcXAndAHE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecoderLSTM(EncoderLSTM):\n",
        "    def __init__(self, TEXT, **kwargs):\n",
        "        super(DecoderLSTM, self).__init__(TEXT, **kwargs)\n",
        "        self.linear = ntorch.nn.Linear(self.hidden_size,\n",
        "                                       self.vocab_size)\n",
        "        self.linear.spec(\"encoding\", \"vocab\")\n",
        "        \n",
        "    def forward(self, input, hidden = None, context = None):\n",
        "        x = self.embeddings(input).relu()\n",
        "        output, (hidden, context) = self.lstm(x, (hidden, context))\n",
        "        output = self.linear(output).log_softmax(\"vocab\")\n",
        "        return output, hidden, context"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWmUizypewzG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Seq2Seq(ntorch.nn.Module):\n",
        "    def __init__(self, SRC_TEXT, TRG_TEXT, embedding_size = 256, \n",
        "                hidden_size = 512, num_layers = 2, dropout=0.0):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = EncoderLSTM(TA, hidden_size=hidden_size,\n",
        "                                   num_layers=num_layers,\n",
        "                                   embedding_size=embedding_size,\n",
        "                                   dropout=dropout)\n",
        "        self.decoder = DecoderLSTM(EN, hidden_size=hidden_size,\n",
        "                                   num_layers=num_layers,\n",
        "                                   embedding_size=embedding_size,\n",
        "                                   dropout=dropout)\n",
        "    \n",
        "    def forward(self, source, target=None, teacher_forcing=0.5,\n",
        "                max_length=20):\n",
        "        if target:\n",
        "            max_length = target.shape[\"seqlen\"]\n",
        "        else:\n",
        "            teacher_forcing = 0\n",
        "        \n",
        "        hidden, context = self.encoder(source)\n",
        "        out_log_probs = ntorch.zeros(source.size(\"batch\"),\n",
        "                                 max_length,\n",
        "                                 self.decoder.vocab_size,\n",
        "                                 names = (\"batch\",\n",
        "                                          \"seqlen\",\n",
        "                                          \"vocab\")).cuda()\n",
        "        input = ntorch.tensor([BOS_ID] * source.size(\"batch\"), names = (\"batch\")).cuda()\n",
        "        input_shape = dict(input.shape)\n",
        "        input_shape[\"seqlen\"] = 1\n",
        "        input = input.split(\"batch\", (\"seqlen\", \"batch\"), **input_shape)\n",
        "        \n",
        "        for t in range(1, max_length):\n",
        "            output, hidden, context = self.decoder(input, hidden, context)\n",
        "            teacher_force = random.random() < teacher_forcing\n",
        "            out_log_probs[{\"seqlen\" : t}] = output.squeeze(\"seqlen\")\n",
        "\n",
        "            if teacher_force:\n",
        "                input = target[{\"seqlen\" : t}]\n",
        "                input = input.split(\"batch\", (\"seqlen\", \"batch\"), **input_shape)\n",
        "            else:\n",
        "                dist = ntorch.distributions.Categorical(logits = output, dim_logit = \"vocab\")\n",
        "                input = dist.sample()\n",
        "                \n",
        "        return out_log_probs         "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GDtDnEfDto7",
        "colab_type": "code",
        "outputId": "3bac3738-de12-4266-e4dc-91b9d83f81f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        }
      },
      "source": [
        "model = Seq2Seq(TA, EN, dropout = 0.01, embedding_size = 128, hidden_size = 256)\n",
        "model.cuda()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): EncoderLSTM(\n",
              "    (dropout): Dropout(p=0.01)\n",
              "    (embeddings): Embedding(13872, 128)\n",
              "    (lstm): LSTM(128, 256, num_layers=2, batch_first=True, dropout=0.01)\n",
              "  )\n",
              "  (decoder): DecoderLSTM(\n",
              "    (dropout): Dropout(p=0.01)\n",
              "    (embeddings): Embedding(12503, 128)\n",
              "    (lstm): LSTM(128, 256, num_layers=2, batch_first=True, dropout=0.01)\n",
              "    (linear): Linear(in_features=256, out_features=12503, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 205
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnjfV5E_bRuY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "PAD_ID = TA.vocab.stoi['<pad>']\n",
        "criterion = ntorch.nn.NLLLoss(ignore_index = PAD_ID).spec(\"vocab\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9NfhCp2eU5S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for i, batch in enumerate(iterator):\n",
        "        source = batch.src\n",
        "        target = batch.trg\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        output = model(source, target)\n",
        "        \n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "    return epoch_loss / len(iterator)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-CfK04vg1aE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(iterator):\n",
        "            source = batch.src\n",
        "            target = batch.trg\n",
        "            \n",
        "            output = model(source, target, teacher_forcing = 0)\n",
        "            \n",
        "            loss = criterion(output, target)\n",
        "            epoch_loss += loss.item()\n",
        "        return epoch_loss / len(iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K234jjEOjD5B",
        "colab_type": "code",
        "outputId": "342a89fc-1464-4282-f8a7-8f873b28fc95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8432471"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 209
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbuXQthFj5V4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZxkbgHbiOQm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGbKbxJ5iZbl",
        "colab_type": "code",
        "outputId": "c3712dfb-77e8-45ff-bb7a-b7c2ed876aad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        }
      },
      "source": [
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(15):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss = train(model, train_iter, optimizer, criterion, 1)\n",
        "    valid_loss = evaluate(model, val_iter, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 4m 11s\n",
            "\tTrain Loss: 5.616 | Train PPL: 274.870\n",
            "\t Val. Loss: 5.556 |  Val. PPL: 258.765\n",
            "Epoch: 02 | Time: 4m 11s\n",
            "\tTrain Loss: 5.262 | Train PPL: 192.962\n",
            "\t Val. Loss: 5.440 |  Val. PPL: 230.330\n",
            "Epoch: 03 | Time: 4m 12s\n",
            "\tTrain Loss: 5.077 | Train PPL: 160.259\n",
            "\t Val. Loss: 5.409 |  Val. PPL: 223.310\n",
            "Epoch: 04 | Time: 4m 12s\n",
            "\tTrain Loss: 4.941 | Train PPL: 139.877\n",
            "\t Val. Loss: 5.345 |  Val. PPL: 209.554\n",
            "Epoch: 05 | Time: 4m 12s\n",
            "\tTrain Loss: 4.817 | Train PPL: 123.558\n",
            "\t Val. Loss: 5.348 |  Val. PPL: 210.144\n",
            "Epoch: 06 | Time: 4m 11s\n",
            "\tTrain Loss: 4.717 | Train PPL: 111.858\n",
            "\t Val. Loss: 5.283 |  Val. PPL: 196.984\n",
            "Epoch: 07 | Time: 4m 12s\n",
            "\tTrain Loss: 4.617 | Train PPL: 101.164\n",
            "\t Val. Loss: 5.302 |  Val. PPL: 200.684\n",
            "Epoch: 08 | Time: 4m 11s\n",
            "\tTrain Loss: 4.526 | Train PPL:  92.417\n",
            "\t Val. Loss: 5.293 |  Val. PPL: 198.901\n",
            "Epoch: 09 | Time: 4m 12s\n",
            "\tTrain Loss: 4.443 | Train PPL:  84.989\n",
            "\t Val. Loss: 5.300 |  Val. PPL: 200.422\n",
            "Epoch: 10 | Time: 4m 11s\n",
            "\tTrain Loss: 4.358 | Train PPL:  78.129\n",
            "\t Val. Loss: 5.333 |  Val. PPL: 206.967\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-212-60f10f611b92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-207-c5c98ee7ab7b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion, clip)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     99\u001b[0m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_exp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                 \u001b[0mbias_correction1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsW_IMDBj90b",
        "colab_type": "code",
        "outputId": "57e566c5-a1ed-4892-9ce9-1605b4c4a3c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "evaluate(model, test_iter, criterion)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5.286605736304974"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 213
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vG6v4gyhMkz6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}